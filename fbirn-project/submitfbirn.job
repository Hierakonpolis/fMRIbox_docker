#!/bin/bash
#SBATCH -n 1
#SBATCH -c 4
#SBATCH --mem=5g
#SBATCH -p qTRD
#SBATCH --time=00:20:00
#SBATCH -J fbirn-test
#SBATCH -e /data/users2/jwardell1/nshor_docker/fbirn-project/jobs/error%A.err
#SBATCH -o /data/users2/jwardell1/nshor_docker/fbirn-project/jobs/out%A.out
#SBATCH -A psy53c17
#SBATCH --mail-type=ALL
#SBATCH --mail-user=jwardell1@student.gsu.edu
#SBATCH --oversubscribe

sleep 5s

module load singularity/3.10.2

#this slurm array goes from zero to num_runs-1
	#where num_runs is the number of (fMRI,sMRI) file pairs to process
	#for FBIRN there is only one run per subject, so slurm array is not needed
	#is needed for other databases though

SUBJECT_ID=$1
SUB_PATHS_FILE=/data/users2/jwardell1/nshor_docker/fbirn-project/fbirn_path_files/${SUBJECT_ID}_paths

SIF_FILE=/data/users2/washbee/tdassist/Dockerbuild/nshor2.sif
RUN_BIND_POINT=/run
SCRIPT_NAME=pd_dockerParallelized_fbirn.sh
OUTPUT_DIRECTORY=/data/users2/jwardell1/nshor_docker/fbirn-project/FBIRN/

SLURM_ARRAY_TASK_ID=0 #set to zero for now since fbirn only has one run, change for other datasets

singularity exec --bind .:$RUN_BIND_POINT $SIF_FILE ${RUN_BIND_POINT}/${SCRIPT_NAME} $SLURM_ARRAY_TASK_ID $SUB_PATHS_FILE $OUTPUT_DIRECTORY $SUBJECT_ID &

wait

sleep 10s
